{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.metrics import (f1_score, auc, roc_curve, confusion_matrix,\n",
    "                             roc_auc_score, matthews_corrcoef)\n",
    "import seaborn as sns\n",
    "mpl.style.use(\"seaborn-deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(\"../results/CNN/binary_class.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Results from DA Detection Paper\n",
    "## Overview\n",
    "This notebook can be run to reproduce all figues and results from the BHK lab's DA detection paper. The raw output from the CNN and the Sinogram-based detection (SBD) algorithm should be saved locally for this notebook to run locally.\n",
    "\n",
    "## Performance Metrics\n",
    "The following metrics are computed for this study.\n",
    "### Binary Classification\n",
    "#### Mathews Correlation Coefficient (MCC)\n",
    "$$ \\text{MCC} = {\\frac {{\\mathit {TP}}\\cdot {\\mathit {TN}}-{\\mathit {FP}}\\cdot {\\mathit {FN}}}{\\sqrt {({\\mathit {TP}}+{\\mathit {FP}})({\\mathit {TP}}+{\\mathit {FN}})({\\mathit {TN}}+{\\mathit {FP}})({\\mathit {TN}}+{\\mathit {FN}})}}} $$\n",
    "\n",
    "#### AUC\n",
    "...\n",
    "\n",
    "\n",
    "### Multiclass Classification\n",
    "#### Generalized MCC\n",
    "$$\n",
    "{\\text{MCC}}={\\frac {\\sum _{k}\\sum _{l}\\sum _{m}C_{kk}C_{lm}-C_{kl}C_{mk}}{{\\sqrt {\\sum _{k}(\\sum _{l}C_{kl})(\\sum _{k'|k'\\neq k}\\sum _{l'}C_{k'l'})}}{\\sqrt {\\sum _{k}(\\sum _{l}C_{lk})(\\sum _{k'|k'\\neq k}\\sum _{l'}C_{l'k'})}}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SBD</th>\n",
       "      <th>CNN</th>\n",
       "      <th>3_class</th>\n",
       "      <th>2_class_annotation</th>\n",
       "      <th>3_class_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TPR</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FPR</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNR</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FNR</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCC</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted AUC</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted F1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              SBD  CNN  3_class  2_class_annotation  3_class_annotation\n",
       "TPR           NaN  NaN      NaN                 NaN                 NaN\n",
       "FPR           NaN  NaN      NaN                 NaN                 NaN\n",
       "TNR           NaN  NaN      NaN                 NaN                 NaN\n",
       "FNR           NaN  NaN      NaN                 NaN                 NaN\n",
       "MCC           NaN  NaN      NaN                 NaN                 NaN\n",
       "Weighted AUC  NaN  NaN      NaN                 NaN                 NaN\n",
       " Weighted F1  NaN  NaN      NaN                 NaN                 NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Get Metrics \"\"\"\n",
    "# Compute metrics for different methods\n",
    "metrics = pd.DataFrame(data={\"SBD\":np.nan, \"CNN\":np.nan, \"3_class\": np.nan, \n",
    "                             \"2_class_annotation\": np.nan, \"3_class_annotation\": np.nan},\n",
    "                       index=[\"TPR\", \"FPR\", \"TNR\", \"FNR\", \"MCC\", \"Weighted AUC\", \" Weighted F1\"])\n",
    "\n",
    "def get_metrics(labels, predictions, scores=None) :\n",
    "    if max(labels) - 2 < 1.0e-5 :\n",
    "        # If the labels are (0, 2), convert to (0, 1)\n",
    "        labels = labels / 2.\n",
    "        predictions = predictions / 2.\n",
    "    \n",
    "    # Compute overall stats\n",
    "    num_y = (labels == 1).sum()\n",
    "    num_n = (labels == 0).sum()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    C = confusion_matrix(labels, predictions)\n",
    "    tn, fp, fn, tp = C.ravel()\n",
    "        \n",
    "    tpr = tp / num_y\n",
    "    fpr = fp / num_n\n",
    "    tnr = tn / num_n\n",
    "    fnr = fn / num_y\n",
    "    \n",
    "    # MCC\n",
    "    mcc = matthews_corrcoef(labels, predictions)\n",
    "    \n",
    "    # AUC\n",
    "    if type(scores) == type(None) :\n",
    "        print(\"No scores provided\")\n",
    "        AUC = roc_auc_score(labels, predictions, average='weighted')\n",
    "    else :\n",
    "        AUC = roc_auc_score(labels, scores, average='weighted')\n",
    "    \n",
    "    # F1 Score\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    return [tpr, fpr, tnr, fnr, mcc, AUC, f1]\n",
    "\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /home/colin/Documents/BHKLab/data/DA_detection_paper/final_results/bhk_preds.csv does not exist: '/home/colin/Documents/BHKLab/data/DA_detection_paper/final_results/bhk_preds.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-301d75c1a768>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Make into DataFrames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbhk_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbhk_results_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"patient_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmat_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_results_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"patient_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/radcure/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/radcure/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/radcure/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/radcure/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/radcure/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /home/colin/Documents/BHKLab/data/DA_detection_paper/final_results/bhk_preds.csv does not exist: '/home/colin/Documents/BHKLab/data/DA_detection_paper/final_results/bhk_preds.csv'"
     ]
    }
   ],
   "source": [
    "\"\"\" Data Loading \"\"\"\n",
    "# Model Output\n",
    "model_output_dir = \"../results/\"\n",
    "sbd_results_path = os.path.join(model_output_dir, \"SBD/binary_class.csv\") # Sinogram-based DA detection \n",
    "cnn_results_path = os.path.join(model_output_dir, \"CNN/binary_class.csv\") # CNN-based DA detection\n",
    "\n",
    "# Make into DataFrames\n",
    "bhk_data = pd.read_csv(bhk_results_path, index_col=\"patient_id\")\n",
    "mat_data = pd.read_csv(cnn_results_path, index_col=\"patient_id\")\n",
    "\n",
    "\n",
    "\n",
    "# PyRadiomic Feature and GTV-DA distance analysis\n",
    "# File locations\n",
    "labels_path = os.path.join(model_output_dir, \"radcure_DA_labels.csv\")\n",
    "double_labels_path = os.path.join(model_output_dir, \"double_labels.csv\")\n",
    "\n",
    "# Dental artifact labels (DAL)\n",
    "df_dal = pd.read_csv(labels_path, index_col=\"patient_id\", \n",
    "                     usecols=[\"patient_id\", \"has_artifact\", \"a_slice\"])\n",
    "da_labels = df_dal\n",
    "\n",
    "# Load results of annotator agreement\n",
    "double_labels = pd.read_csv(double_labels_path, index_col=\"patient_id\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" SBD and Thresholding-based Location detection\"\"\"\n",
    "threshld_loc_preds = os.path.join(model_output_dir, \"loc_preds.json\")\n",
    "sinogram_loc_preds = os.path.join(model_output_dir, \"bhk_preds.json\")\n",
    "import json\n",
    "with open(threshld_loc_preds) as json_file:\n",
    "    thr_loc_data = json.load(json_file)\n",
    "with open(sinogram_loc_preds) as json_file:\n",
    "    sin_loc_data = json.load(json_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data Cleaning\"\"\"\n",
    "# Combine all results into one data frame with Radiomic Features, CNN, SBD, labels\n",
    "# Combine the results from Mattea and Colin's methods into one DF\n",
    "\n",
    "\"\"\" CNN and SBD DA classification outputs \"\"\"\n",
    "mat_data[\"bhk_preds\"] = bhk_data[\"prediction\"]\n",
    "mat_data[\"CNN_preds\"] = mat_data[\"CNN_preds\"] * 2\n",
    "# mat_data = mat_data.drop(columns=[\"p_index\", \"manual_artifact_location\"])\n",
    "\n",
    "full_data = mat_data\n",
    "pred_data = mat_data.dropna()\n",
    "\n",
    "# Rename some columns\n",
    "pred_data = pred_data.rename(columns={\"manual_artifact_status\": \"DA_mag\",\n",
    "                                      \"CNN_preds\": \"CNN_pred\", \"CNN_probs0\":\"CNN_prob0\",\n",
    "                                      \"CNN_probs1\":\"CNN_prob1\", \"bhk_preds\":\"bhk_pred\"})\n",
    "# Add column for binary label\n",
    "pred_data[\"bit_label\"] = (pred_data[\"DA_mag\"] > 0).astype(int) * 2\n",
    "\n",
    "\n",
    "pred_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Compute some statistics for human classification agreement \"\"\"\n",
    "\n",
    "print(\"Number of patients with 2 human labels: \", len(double_labels))\n",
    "\n",
    "# Binarize the labels ((1 or 2) == 1 and 0 == 0)\n",
    "binA = (double_labels[\"A_label\"] != 0).values.astype(int)\n",
    "binB = (double_labels[\"B_label\"] != 0).values.astype(int)\n",
    "\n",
    "\n",
    "# print(get_metrics(double_labels[\"A_binary\"].values, double_labels[\"B_binary\"].values))\n",
    "\"\"\" Mathews Correlation for Binary agreement \"\"\"\n",
    "metrics.loc[\"MCC\", \"2_class_annotation\"] = matthews_corrcoef(binA, binB)\n",
    "\n",
    "\"\"\" Matthews correlation coefficient for 3 class agreement\"\"\"\n",
    "metrics.loc[\"MCC\", \"3_class_annotation\"] = matthews_corrcoef(double_labels[\"A_label\"].values, \n",
    "                                                             double_labels[\"B_label\"].values)\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Sinogram Based Detection Evaluation \"\"\"\n",
    "metrics[\"SBD\"] = get_metrics(pred_data[\"bit_label\"].values, pred_data[\"bhk_pred\"].values)\n",
    "\n",
    "\n",
    "# Compute some stats about this data\n",
    "print(\"Number of Images tested on:\", len(pred_data[\"bhk_pred\"]))\n",
    "print(\"Number of Strong: \", sum(pred_data[\"DA_mag\"] == 2))\n",
    "print(\"Number of Weak: \",   sum(pred_data[\"DA_mag\"] == 1))\n",
    "print(\"Number of no DA: \",  sum(pred_data[\"DA_mag\"] == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Sinogram-Based DA Location Detection and Thresholding DA location detection \"\"\"\n",
    "thr_loc_pred = []   # Contains a location prediction for every DA+ patient\n",
    "sin_loc_pred = []   # COntains a location prediction for every img predictied as DA+ by SBD\n",
    "loc_pids = []\n",
    "loc_labels = []\n",
    "\n",
    "labels = da_labels[\"a_slice\"]\n",
    "\n",
    "for key in thr_loc_data :\n",
    "    \n",
    "    thr_preds = thr_loc_data[key]\n",
    "    sin_preds = sin_loc_data[key]\n",
    "    label = int(labels.loc[int(key)] )\n",
    "    \n",
    "    # Thresholding location prediction\n",
    "    if len(thr_preds) > 1 :\n",
    "        # Find pred which is closest to label\n",
    "        deltas = np.abs(np.array(thr_preds) - label)\n",
    "        best_pred = thr_preds[np.argmin(deltas)]\n",
    "        thr_loc_pred.append(best_pred)\n",
    "    else :\n",
    "        thr_loc_pred.append(thr_preds[0])\n",
    "        \n",
    "    # Sinogram location prediction\n",
    "    if len(sin_preds) > 1 :\n",
    "        sin_deltas = np.abs(np.array(sin_preds)+20 - label)\n",
    "        sin_best_pred = sin_preds[np.argmin(sin_deltas)]+20\n",
    "        sin_loc_pred.append(best_pred)\n",
    "    elif len(sin_preds) == 1 :\n",
    "        sin_loc_pred.append(sin_preds[0]+20)\n",
    "    else :\n",
    "        sin_loc_pred.append(np.nan)\n",
    "    \n",
    "\n",
    "    loc_pids.append(key)\n",
    "    loc_labels.append(label)\n",
    "\n",
    "    \n",
    "    \n",
    "loc_df = pd.DataFrame({\"patient_id\": loc_pids, \n",
    "                       \"label\": loc_labels, \n",
    "                       \"thr_pred\": thr_loc_pred,\n",
    "                       \"sin_pred\": sin_loc_pred}).set_index(\"patient_id\")\n",
    "\n",
    "loc_df[\"thr_delta\"] = loc_df[\"thr_pred\"] - loc_df[\"label\"]\n",
    "loc_df[\"sin_delta\"] = loc_df[\"sin_pred\"] - loc_df[\"label\"]\n",
    "        \n",
    "loc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at results\n",
    "n_thr = len(loc_df)\n",
    "exact = len(loc_df[loc_df[\"thr_delta\"] == 0])         / n_thr\n",
    "pm_5  = len(loc_df[np.abs(loc_df[\"thr_delta\"]) < 5])  / n_thr\n",
    "pm_10 = len(loc_df[np.abs(loc_df[\"thr_delta\"]) < 10]) / n_thr\n",
    "pm_15 = len(loc_df[np.abs(loc_df[\"thr_delta\"]) < 15]) / n_thr\n",
    "print(\"THRESHOLDING BASED LOCATION\")\n",
    "print(\"Tested on \", len(loc_df[\"thr_pred\"].dropna()), \"images\")\n",
    "print(exact, pm_5, pm_10, pm_15)\n",
    "print(\"Mean:\", loc_df[\"thr_delta\"].mean())\n",
    "print(\"Std:\", loc_df[\"thr_delta\"].std())\n",
    "\n",
    "# Look at sinogram-based results\n",
    "n_sin = len(loc_df[\"sin_delta\"].dropna())\n",
    "exact = len(loc_df[loc_df[\"sin_delta\"] == 0])         / n_sin\n",
    "pm_5  = len(loc_df[np.abs(loc_df[\"sin_delta\"]) < 5])  / n_sin\n",
    "pm_10 = len(loc_df[np.abs(loc_df[\"sin_delta\"]) < 10]) / n_sin\n",
    "pm_15 = len(loc_df[np.abs(loc_df[\"sin_delta\"]) < 15]) / n_sin\n",
    "print(\"\\nSINOGRAM BASED LOCATION\")\n",
    "print(\"Tested on \", len(loc_df[\"sin_pred\"].dropna()), \"images\")\n",
    "print(exact, pm_5, pm_10, pm_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" Make graphs based on above results \"\"\"\n",
    "\n",
    "\n",
    "# # Create list of percentages for each distance\n",
    "# labeller, n_double = [], len(double_labels)\n",
    "# label_diff = double_labels[\"raw_diff\"]\n",
    "# sin_diff, thr_diff = [], []\n",
    "\n",
    "# limit = 40\n",
    "\n",
    "# for i in range(0, limit) :\n",
    "#     if i == 0 :\n",
    "#         labeller.append(len(label_diff[label_diff == i].abs()) / n_double)\n",
    "#         sin_diff.append(len(loc_df[loc_df[\"sin_delta\"].abs() == i]) / n_sin)\n",
    "#         thr_diff.append(len(loc_df[loc_df[\"thr_delta\"].abs() == i]) / n_thr)\n",
    "\n",
    "#     else :\n",
    "#         labeller.append(len(label_diff[label_diff < i].abs()) / n_double)\n",
    "#         sin_diff.append(len(loc_df[loc_df[\"sin_delta\"].abs() < i]) / n_sin)\n",
    "#         thr_diff.append(len(loc_df[loc_df[\"thr_delta\"].abs() < i]) / n_thr)\n",
    "        \n",
    "# # divide all data by number of samples to get percentages\n",
    "# mpl.style.use(\"classic\")\n",
    "# fig = plt.figure()\n",
    "# fig.set_facecolor('white')\n",
    "# plt.plot(np.arange(0, limit), labeller, \"--\", label=\"Labeller Agreement\")\n",
    "# plt.plot(np.arange(0, limit), sin_diff, label=\"Sinogram-based detection\")\n",
    "# plt.plot(np.arange(0, limit), thr_diff, label=\"Threshold-based detection\")\n",
    "# plt.title(\"Comparison of DA slice detection methods with labeller agreement\")\n",
    "# plt.xlabel(\"Distance from prediction to label [slices]\")\n",
    "# plt.ylabel(\"Proportion of Images With Agreeing DA Location\")\n",
    "# plt.legend(loc=\"center right\")\n",
    "# plt.ylim([0, 1])\n",
    "# plt.xlim([0, 25])\n",
    "# plt.xticks(np.arange(0, 25))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Violin plot for location data \"\"\"\n",
    "thr = loc_df[\"thr_delta\"].dropna().values\n",
    "sin = loc_df[\"sin_delta\"].dropna().values\n",
    "agr = double_labels[\"raw_diff\"].values\n",
    "\n",
    "means = [thr.mean(), sin.mean(), agr.mean()]\n",
    "error = [thr.std(), sin.std(), agr.std()]\n",
    "# mpl.style.use(\"classic\")\n",
    "plt.rc('font', family='sans-serif', size=15)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "ax.violinplot([thr, sin, agr], [0,0.5,1], points=300, widths=0.3,\n",
    "                      showmeans=False, showextrema=False, showmedians=True)\n",
    "ax.set_xticks([0,0.5,1])\n",
    "ax.set_xticklabels([\"Threshold-based\", \"Sinogram-based\", \"Annotator agreement\"])\n",
    "ax.set_ylabel(\"Number of slices between prediction and label\")\n",
    "ax.set_title(\"Comparison of DA location detectors\")\n",
    "plt.rc('font', family='serif', size=15)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Seaborn Violin plots \"\"\"\n",
    "# mpl.style.use(\"classic\")\n",
    "# mpl.style.use(\"classic\")\n",
    "plt.rc('font', family='sans-serif', size=15)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=[6, 4])\n",
    "fig.set_facecolor('white')\n",
    "ax = sns.violinplot(data=[thr, sin, agr])\n",
    "ax.set_xticklabels([\"Threshold-based\", \"Sinogram-based\", \"Annotator\\nagreement\"])\n",
    "ax.set_ylabel(\"Slices between prediction and label\")\n",
    "ax.set_xlabel(\"Detection Method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Convolutional Neural Network Detection \"\"\"\n",
    "\n",
    "# Remove images that were used in CNN training\n",
    "trg_data = pd.read_csv(\"/home/colin/Downloads/CNN_Full_preds.csv\", \n",
    "                       usecols=[\"patient_id\", \"isDuplicate\"], \n",
    "                       index_col=\"patient_id\")\n",
    "test_data = pred_data[trg_data.loc[:, \"isDuplicate\"] == False]\n",
    "\n",
    "# Print Some stats\n",
    "print(\"Number of CNN test Images Used: \", len(test_data))\n",
    "print(\"Number of Strong: \", sum(test_data[\"DA_mag\"] == 2))\n",
    "print(\"Number of Weak: \", sum(test_data[\"DA_mag\"] == 1))\n",
    "print(\"Number of no DA: \", sum(test_data[\"DA_mag\"] == 0))\n",
    "\n",
    "# Get performance metrics\n",
    "metrics[\"CNN\"] = get_metrics(test_data[\"bit_label\"].values, \n",
    "                             test_data[\"CNN_pred\"].values, \n",
    "                             scores=test_data[\"CNN_prob1\"].values)\n",
    "\n",
    "# mpl.style.use(\"classic\")\n",
    "\n",
    "\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(test_data[\"bit_label\"].values, \n",
    "                                 test_data[\"CNN_prob1\"].values,\n",
    "                                 pos_label=2)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('white')\n",
    "plt.rc('font', family='sans-serif', size=15)\n",
    "plt.rc('xtick', labelsize='x-small')\n",
    "plt.rc('ytick', labelsize='x-small')\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"AUC = \"+str(metrics.loc[\"Weighted AUC\", \"CNN\"])[:5])\n",
    "plt.plot([0,0.5, 1], [0, 0.5, 1], label=\"Random\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Binary CNN ROC Curve\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.show()\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a 3-Class Classifier\n",
    "![alt text](3_class_schematic.jpg \"Schematic for a 3-class DA classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 3-Class DA Classifier Results \"\"\"\n",
    "# Algorithm for 3 class\n",
    "multi_pred = []\n",
    "for pid in test_data.index.values :   # Use CNN test data\n",
    "    sinogram = test_data.loc[pid, \"bhk_pred\"]\n",
    "    cnn = test_data.loc[pid, \"CNN_pred\"]\n",
    "    \n",
    "    # Sinogram first\n",
    "    if sinogram == 2 :\n",
    "        if cnn == 2 : \n",
    "            multi_pred.append(2)\n",
    "        else : # CNN = 0\n",
    "            multi_pred.append(0)\n",
    "    else : # Sinogram = 0\n",
    "        if cnn == 2 :\n",
    "            multi_pred.append(1)\n",
    "        else : # CNN = 0\n",
    "            multi_pred.append(0)\n",
    "            \n",
    "\n",
    "test_data.loc[:, \"multi_pred\"] = multi_pred\n",
    "\n",
    "# Print stats\n",
    "print(\"Number of test images: \", len(test_data))\n",
    "print(\"Number of strong\", sum(test_data[\"DA_mag\"] == 2))\n",
    "print(\"Number of weak\", sum(test_data[\"DA_mag\"] == 1))\n",
    "print(\"Number of none\", sum(test_data[\"DA_mag\"] == 0), \"\\n\")\n",
    "\n",
    "print(\"MCC For 3-Class Classifier\")\n",
    "three_class_mcc = matthews_corrcoef(test_data[\"DA_mag\"], test_data[\"multi_pred\"])\n",
    "print(three_class_mcc)\n",
    "\n",
    "# Add 3 class MCC to metrics df\n",
    "metrics.loc[\"MCC\", \"3_class\"] = three_class_mcc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib as mpl\n",
    "# mpl.style.use('fast')\n",
    "# mpl.style.use(\"classic\")\n",
    "\n",
    "\n",
    "plt.close('all')\n",
    "plt.rc('font', family='sans-serif', size=15)\n",
    "plt.rc('xtick', labelsize='x-small')\n",
    "plt.rc('ytick', labelsize='x-small')\n",
    "fig, ax = plt.subplots(figsize=[6, 4], facecolor=\"white\")\n",
    "\n",
    "metrics.loc[\"MCC\", :].plot.bar(color=[\"#59A14F\", \"#ECC539\", \"#AA6A99\", \"#519DBC\", \"#F08333\"], \n",
    "                               ax=ax)\n",
    "# metrics.loc[\"MCC\", :].plot.bar()\n",
    "plt.ylabel(\"Matthews Correlation Coeficient (MCC)\", fontsize=15)\n",
    "plt.xlabel(\"Classification Method\", fontsize=15)\n",
    "plt.xticks([0, 1, 2, 3, 4], \n",
    "           [\"SBD\", \"CNN\", \"Hybrid\", \"Annotator\\n(binary)\", \"Annotator\\n(3-class)\"])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create other contingency tables\"\"\"\n",
    "# Labeller agreement\n",
    "print(\"Labeller Agreement\")\n",
    "print(pd.crosstab(double_labels[\"A_label\"], double_labels[\"B_label\"]), \"\\n\")\n",
    "\n",
    "# Sinogram detection\n",
    "print(\"Sinogram binary detection\")\n",
    "print(pd.crosstab(pred_data[\"bit_label\"], pred_data[\"bhk_pred\"]), \"\\n\")\n",
    "\n",
    "# CNN detection\n",
    "print(\"CNN binary detection\")\n",
    "print(pd.crosstab(test_data[\"bit_label\"], test_data[\"CNN_pred\"] ), \"\\n\")\n",
    "\n",
    "# Hybrid 3-class detection\n",
    "print(\"Hybrid 3-class detection\")\n",
    "print(pd.crosstab(test_data[\"DA_mag\"], test_data[\"multi_pred\"]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RadCure",
   "language": "python",
   "name": "radcure"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
