{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.metrics import (f1_score, auc, roc_curve, confusion_matrix,\n",
    "                             roc_auc_score, matthews_corrcoef)\n",
    "import seaborn as sns\n",
    "mpl.style.use(\"seaborn-deep\")\n",
    "\n",
    "\n",
    "\n",
    "from typing import Tuple, Callable, Dict, Optional, Union, List\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(\"../results/CNN/binary_class.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Results from DA Detection Paper\n",
    "## Overview\n",
    "This notebook can be run to reproduce all figues and results from the BHK lab's DA detection paper. The raw output from the CNN and the Sinogram-based detection (SBD) algorithm should be saved locally for this notebook to run locally.\n",
    "\n",
    "## Performance Metrics\n",
    "The following metrics are computed for this study.\n",
    "### Binary Classification\n",
    "#### Mathews Correlation Coefficient (MCC)\n",
    "$$ \\text{MCC} = {\\frac {{\\mathit {TP}}\\cdot {\\mathit {TN}}-{\\mathit {FP}}\\cdot {\\mathit {FN}}}{\\sqrt {({\\mathit {TP}}+{\\mathit {FP}})({\\mathit {TP}}+{\\mathit {FN}})({\\mathit {TN}}+{\\mathit {FP}})({\\mathit {TN}}+{\\mathit {FN}})}}} $$\n",
    "\n",
    "#### AUC\n",
    "...\n",
    "\n",
    "\n",
    "### Multiclass Classification\n",
    "#### Generalized MCC\n",
    "$$\n",
    "{\\text{MCC}}={\\frac {\\sum _{k}\\sum _{l}\\sum _{m}C_{kk}C_{lm}-C_{kl}C_{mk}}{{\\sqrt {\\sum _{k}(\\sum _{l}C_{kl})(\\sum _{k'|k'\\neq k}\\sum _{l'}C_{k'l'})}}{\\sqrt {\\sum _{k}(\\sum _{l}C_{lk})(\\sum _{k'|k'\\neq k}\\sum _{l'}C_{l'k'})}}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_ci(y_true: np.ndarray,\n",
    "                 y_pred: np.ndarray,\n",
    "                 metric: Callable[[np.ndarray, np.ndarray], float],\n",
    "                 n_iterations: int = 5000,\n",
    "                 n_samples: int = 200,\n",
    "                 n_jobs: int = -1,\n",
    "                 **kwargs) -> Tuple[float, float]:\n",
    "    \"\"\" Compute the confidence intervals using bootstrapping.\n",
    "    \n",
    "    Randomly select s subsets of the test set and calculate a distrubution of \n",
    "    the metric on those sets.\n",
    "    \n",
    "    Parameters :\n",
    "    ------------\n",
    "    y_true : np.ndarray, shape=(n_samples,)\n",
    "        The ground truth values.\n",
    "    y_pred : np.ndarray, shape=(n_samples,)\n",
    "        The model predictions.\n",
    "    metric :\n",
    "        The performance metric.\n",
    "    n_iterations : int, optional\n",
    "        Number of times to sample the test set.\n",
    "    n_samples : int, optional\n",
    "        Number of samples to take in each iteration.\n",
    "    n_jobs : int, optional\n",
    "        Number of parallel processes to use.\n",
    "    **kwargs\n",
    "        Additional keyword arguments to pass to metric\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of two floats\n",
    "        The upper and lower 95% confidence interval.\n",
    "    \"\"\"\n",
    "    \n",
    "    def inner() :\n",
    "        sample_y_true, sample_y_pred = resample(y_true, y_pred, n_samples=n_samples)\n",
    "        return metric(sample_y_true, sample_y_pred, **kwargs)\n",
    "    estimate = metric(y_true, y_pred, **kwargs)\n",
    "    boot_estimates = Parallel(n_jobs=n_jobs)(delayed(inner)() for _ in range(n_iterations))\n",
    "    boot_estimates = np.array(boot_estimates)\n",
    "    lower = max(0.0, np.percentile(boot_estimates, 2.5))\n",
    "    upper = min(1.0, np.percentile(boot_estimates, 97.5))\n",
    "    return estimate, lower, upper\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(y_true: np.ndarray,\n",
    "                     y_pred: np.ndarray,\n",
    "                     metric: Callable[[np.ndarray, np.ndarray], float],\n",
    "                     n_permutations: int = 5000,\n",
    "                     n_jobs: int = -1,\n",
    "                     **kwargs) -> Tuple[float, float]:\n",
    "    r\"\"\"Compute significance of predictions using a randomized permutation test.\n",
    "\n",
    "    The p value is computed as\n",
    "    ``1/(N+1) * (sum(s(y, y_pred) >= s(perm(y), perm(y_pred)) for _ in range(N)) + 1)``\n",
    "    where `s` is the performance metric and `perm` denotes a random permutation. In words,\n",
    "    it is the estimated probability that a random prediction would give score at least\n",
    "    as good as the actual prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray, shape=(n_samples,)\n",
    "        The ground truth values.\n",
    "    y_pred : np.ndarray, shape=(n_samples,)\n",
    "        The model predictions.\n",
    "    metric\n",
    "        The performance metric.\n",
    "    n_permutations, optional\n",
    "        How many random permutations to use. Larger values give more\n",
    "        accurate estimates but take longer to run.\n",
    "    n_jobs, optional\n",
    "        Number of parallel processes to use.\n",
    "    **kwargs\n",
    "        Additional keyword arguments passed to metric.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of 2 floats\n",
    "        The value of the performance metric and the estimated p value.\n",
    "    \"\"\"\n",
    "    def inner():\n",
    "        return metric(np.random.permutation(y_true), np.random.permutation(y_pred), **kwargs)\n",
    "    estimate = metric(y_true, y_pred, **kwargs)\n",
    "    perm_estimates = Parallel(n_jobs=n_jobs)(delayed(inner)() for _ in range(n_permutations))\n",
    "    perm_estimates = np.array(perm_estimates)\n",
    "    pval = ((perm_estimates >= estimate).sum() + 1) / (n_permutations + 1)\n",
    "\n",
    "    return estimate, pval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SBD</th>\n",
       "      <th>CNN</th>\n",
       "      <th>3_class</th>\n",
       "      <th>2_class_annotation</th>\n",
       "      <th>3_class_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TPR</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FPR</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TNR</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FNR</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCC</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCC CI_l</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCC CI_u</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SBD  CNN  3_class  2_class_annotation  3_class_annotation\n",
       "TPR       NaN  NaN      NaN                 NaN                 NaN\n",
       "FPR       NaN  NaN      NaN                 NaN                 NaN\n",
       "TNR       NaN  NaN      NaN                 NaN                 NaN\n",
       "FNR       NaN  NaN      NaN                 NaN                 NaN\n",
       "MCC       NaN  NaN      NaN                 NaN                 NaN\n",
       "MCC CI_l  NaN  NaN      NaN                 NaN                 NaN\n",
       "MCC CI_u  NaN  NaN      NaN                 NaN                 NaN\n",
       "AUC       NaN  NaN      NaN                 NaN                 NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Get Metrics \"\"\"\n",
    "# Compute metrics for different methods\n",
    "metrics = pd.DataFrame(data={\"SBD\":np.nan, \"CNN\":np.nan, \"3_class\": np.nan, \n",
    "                             \"2_class_annotation\": np.nan, \"3_class_annotation\": np.nan},\n",
    "                       index=[\"TPR\", \"FPR\", \"TNR\", \"FNR\", \"MCC\", \"MCC CI_l\", \"MCC CI_u\",\"AUC\"])\n",
    "\n",
    "\n",
    "def tpr(labels, predictions) :\n",
    "    \"\"\" True positive rate\"\"\"\n",
    "    num_y = (labels == 1).sum()\n",
    "    C = confusion_matrix(labels, predictions)\n",
    "    tn, fp, fn, tp = C.ravel()\n",
    "\n",
    "    return tp / num_y\n",
    "\n",
    "\n",
    "def fpr(labels, predictions) :\n",
    "    \"\"\" False positive rate \"\"\"\n",
    "    num_n = (labels == 0).sum()\n",
    "    C = confusion_matrix(labels, predictions)\n",
    "    tn, fp, fn, tp = C.ravel()\n",
    "\n",
    "    return fp / num_n\n",
    "\n",
    "def tnr(labels, predictions):\n",
    "    \"\"\" true negative rate \"\"\"\n",
    "    # Compute overall stats\n",
    "    num_n = (labels == 0).sum()\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    C = confusion_matrix(labels, predictions)\n",
    "    tn, fp, fn, tp = C.ravel()\n",
    "    return tn / num_n\n",
    "\n",
    "\n",
    "def fnr(labels, predictions) :\n",
    "    \"\"\" False negative rate \"\"\"\n",
    "    num_y = (labels == 1).sum()\n",
    "    C = confusion_matrix(labels, predictions)\n",
    "    tn, fp, fn, tp = C.ravel()\n",
    "    return fn / num_y\n",
    "\n",
    "\n",
    "def get_metrics(labels, predictions, scores=None) :\n",
    "    if max(labels) - 2 < 1.0e-5 :\n",
    "        # If the labels are (0, 2), convert to (0, 1)\n",
    "        labels = labels / 2.\n",
    "        predictions = predictions / 2.\n",
    "    \n",
    "#     # Compute overall stats\n",
    "#     num_y = (labels == 1).sum()\n",
    "#     num_n = (labels == 0).sum()\n",
    "    \n",
    "#     # Compute confusion matrix\n",
    "#     C = confusion_matrix(labels, predictions)\n",
    "#     tn, fp, fn, tp = C.ravel()\n",
    "        \n",
    "#     tpr = tp / num_y\n",
    "#     fpr = fp / num_n\n",
    "#     tnr = tn / num_n\n",
    "#     fnr = fn / num_y\n",
    "    TPR, TPR_ci1, TPR_ci2 = bootstrap_ci(labels, predictions, tpr)\n",
    "    FPR, FPR_ci1, FPR_ci2 = bootstrap_ci(labels, predictions, fpr)\n",
    "    TNR, TNR_ci1, TNR_ci2 = bootstrap_ci(labels, predictions, tnr)\n",
    "    FNR, FNR_ci1, FNR_ci2 = bootstrap_ci(labels, predictions, fnr)\n",
    "        \n",
    "    # MCC\n",
    "    MCC, MCC_p = permutation_test(labels, predictions, matthews_corrcoef)\n",
    "    _, MCC_ci1, MCC_ci2 = bootstrap_ci(labels, predictions, matthews_corrcoef)\n",
    "    \n",
    "    # AUC\n",
    "    if scores is None :\n",
    "        print(\"No scores provided\")\n",
    "        AUC, AUC_p = permutation_test(labels, predictions, \n",
    "                                              roc_auc_score, average='weighted')\n",
    "        _, AUC_ci1, AUC_ci2 = bootstrap_ci(labels, predictions, \n",
    "                                          roc_auc_score, average='weighted')\n",
    "    else :       \n",
    "        AUC, AUC_p = permutation_test(labels, scores, \n",
    "                                              roc_auc_score, average='weighted')\n",
    "        _, AUC_ci1, AUC_ci2 = bootstrap_ci(labels, scores, \n",
    "                                          roc_auc_score, average='weighted')\n",
    "\n",
    "        \n",
    "    \n",
    "    # F1 Score\n",
    "#     F1, F1_p = permutation_test(labels, predictions, f1_score, average='weighted')\n",
    "    \n",
    "    print(\"TPR: \", TPR, r\"$\\pm$\", TPR_ci1, TPR_ci2)\n",
    "    print(\"FPR: \", FPR, r\"$\\pm$\", FPR_ci1, FPR_ci2)\n",
    "    print('TNR: ', TNR, r\"$\\pm$\", TNR_ci1, TNR_ci2)\n",
    "    print('FNR: ', FNR, r\"$\\pm$\", FNR_ci1, FNR_ci2)\n",
    "    print('AUC: ', AUC, r\"$\\pm$\", AUC_ci1, AUC_ci2, \", p=\", AUC_p)\n",
    "    print('MCC: ', MCC, r\"$\\pm$\", MCC_ci1, MCC_ci2, \", p=\", MCC_p)    \n",
    "    \n",
    "    return [TPR, FPR, TNR, FNR, MCC, MCC_ci1, MCC_ci2, AUC]\n",
    "\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data Loading \"\"\"\n",
    "# Model Output\n",
    "model_output_dir = \"../results/\"\n",
    "sbd_results_path = os.path.join(model_output_dir, \"SBD/binary_class.csv\") # Sinogram-based DA detection \n",
    "cnn_results_path = os.path.join(model_output_dir, \"CNN/binary_class.csv\") # CNN-based DA detection\n",
    "\n",
    "# Make into DataFrames\n",
    "bhk_data = pd.read_csv(sbd_results_path, index_col=\"patient_id\")\n",
    "mat_data = pd.read_csv(cnn_results_path, index_col=\"patient_id\")\n",
    "\n",
    "\n",
    "\n",
    "# PyRadiomic Feature and GTV-DA distance analysis\n",
    "# File locations\n",
    "data_path = \"../../data\"\n",
    "labels_path = os.path.join(data_path, \"radcure_DA_labels.csv\")\n",
    "double_labels_path = os.path.join(data_path, \"double_labels.csv\")\n",
    "\n",
    "# Dental artifact labels (DAL)\n",
    "df_dal = pd.read_csv(labels_path, index_col=\"patient_id\", \n",
    "                     usecols=[\"patient_id\", \"has_artifact\", \"a_slice\"])\n",
    "da_labels = df_dal\n",
    "\n",
    "# Load results of annotator agreement\n",
    "double_labels = pd.read_csv(double_labels_path, index_col=\"patient_id\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" SBD and Thresholding-based Location detection\"\"\"\n",
    "threshld_loc_preds = os.path.join(model_output_dir, \"thresholding/loc_preds.json\")\n",
    "sinogram_loc_preds = os.path.join(model_output_dir, \"SBD/locations.json\")\n",
    "import json\n",
    "with open(threshld_loc_preds) as json_file:\n",
    "    thr_loc_data = json.load(json_file)\n",
    "with open(sinogram_loc_preds) as json_file:\n",
    "    sin_loc_data = json.load(json_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_index</th>\n",
       "      <th>CNN_pred</th>\n",
       "      <th>CNN_prob0</th>\n",
       "      <th>CNN_prob1</th>\n",
       "      <th>bhk_pred</th>\n",
       "      <th>DA_mag</th>\n",
       "      <th>bit_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patient_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3754620</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999510</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3542698</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.024926</td>\n",
       "      <td>0.975074</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3811942</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027863</td>\n",
       "      <td>0.972137</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3985854</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3836493</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.008074</td>\n",
       "      <td>0.991926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761928</th>\n",
       "      <td>3206</td>\n",
       "      <td>0</td>\n",
       "      <td>0.994663</td>\n",
       "      <td>0.005337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3396249</th>\n",
       "      <td>3207</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.999977</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3715699</th>\n",
       "      <td>3208</td>\n",
       "      <td>0</td>\n",
       "      <td>0.998144</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054092</th>\n",
       "      <td>3209</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999687</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089782</th>\n",
       "      <td>3210</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002409</td>\n",
       "      <td>0.997591</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3211 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            p_index  CNN_pred  CNN_prob0  CNN_prob1  bhk_pred  DA_mag  \\\n",
       "patient_id                                                              \n",
       "3754620           0         0   0.999510   0.000490         0       0   \n",
       "3542698           1         2   0.024926   0.975074         2       2   \n",
       "3811942           2         2   0.027863   0.972137         2       2   \n",
       "3985854           3         0   0.999996   0.000004         0       0   \n",
       "3836493           4         2   0.008074   0.991926         0       1   \n",
       "...             ...       ...        ...        ...       ...     ...   \n",
       "3761928        3206         0   0.994663   0.005337         0       0   \n",
       "3396249        3207         2   0.000023   0.999977         2       2   \n",
       "3715699        3208         0   0.998144   0.001856         0       0   \n",
       "4054092        3209         0   0.999687   0.000313         0       0   \n",
       "2089782        3210         2   0.002409   0.997591         0       1   \n",
       "\n",
       "            bit_label  \n",
       "patient_id             \n",
       "3754620             0  \n",
       "3542698             2  \n",
       "3811942             2  \n",
       "3985854             0  \n",
       "3836493             2  \n",
       "...               ...  \n",
       "3761928             0  \n",
       "3396249             2  \n",
       "3715699             0  \n",
       "4054092             0  \n",
       "2089782             2  \n",
       "\n",
       "[3211 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Data Cleaning\"\"\"\n",
    "# Combine all results into one data frame with Radiomic Features, CNN, SBD, labels\n",
    "# Combine the results from Mattea and Colin's methods into one DF\n",
    "\n",
    "\"\"\" CNN and SBD DA classification outputs \"\"\"\n",
    "mat_data[\"bhk_preds\"] = bhk_data[\"prediction\"]\n",
    "mat_data[\"CNN_preds\"] = mat_data[\"CNN_preds\"] * 2\n",
    "mat_data[\"da_label\"] = da_labels[\"has_artifact\"]\n",
    "# mat_data = mat_data.drop(columns=[\"p_index\", \"manual_artifact_location\"])\n",
    "\n",
    "full_data = mat_data\n",
    "pred_data = mat_data.dropna()\n",
    "\n",
    "# Rename some columns\n",
    "pred_data = pred_data.rename(columns={\"da_label\": \"DA_mag\",\n",
    "                                      \"CNN_preds\": \"CNN_pred\", \"CNN_probs0\":\"CNN_prob0\",\n",
    "                                      \"CNN_probs1\":\"CNN_prob1\", \"bhk_preds\":\"bhk_pred\"})\n",
    "# Add column for binary label\n",
    "pred_data[\"bit_label\"] = (pred_data[\"DA_mag\"] > 0).astype(int) * 2\n",
    "\n",
    "\n",
    "pred_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients with 2 human labels:  482\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ebe09bf49926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\"\"\" Mathews Correlation for Binary agreement \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MCC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2_class_annotation\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutation_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatthews_corrcoef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m metrics.loc[\"MCC CI\", \"2_class_annotation\"] = np.array(bootstrap_ci(binA, binB, \n\u001b[0m\u001b[1;32m     14\u001b[0m                                                                     matthews_corrcoef)[1:])\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbootstrap_ci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmatthews_corrcoef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0;31m# actually do the set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"setitem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mputmask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, filter, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36msetitem\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;31m# set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "\"\"\" Compute some statistics for human classification agreement \"\"\"\n",
    "\n",
    "print(\"Number of patients with 2 human labels: \", len(double_labels))\n",
    "\n",
    "# Binarize the labels ((1 or 2) == 1 and 0 == 0)\n",
    "binA = (double_labels[\"A_label\"] != 0).values.astype(int)\n",
    "binB = (double_labels[\"B_label\"] != 0).values.astype(int)\n",
    "\n",
    "\n",
    "# print(get_metrics(double_labels[\"A_binary\"].values, double_labels[\"B_binary\"].values))\n",
    "\"\"\" Mathews Correlation for Binary agreement \"\"\"\n",
    "metrics.loc[\"MCC\", \"2_class_annotation\"] = permutation_test(binA, binB, matthews_corrcoef)[0]\n",
    "metrics.loc[[\"MCC CI_l\", \"MCC_CI_u\"], \"2_class_annotation\"] = np.array(bootstrap_ci(binA, binB, \n",
    "                                                                    matthews_corrcoef)[1:])\n",
    "print(np.array(bootstrap_ci(binA, binB,matthews_corrcoef)[1:]))\n",
    "\n",
    "\"\"\" Matthews correlation coefficient for 3 class agreement\"\"\"\n",
    "metrics.loc[\"MCC\", \"3_class_annotation\"] = permutation_test(double_labels[\"A_label\"].values, \n",
    "                                                            double_labels[\"B_label\"].values,\n",
    "                                                            matthews_corrcoef)[0]\n",
    "metrics.loc[[\"MCC CI_l\", \"MCC_CI_u\"], \"3_class_annotation\"] = np.array(\n",
    "                                             bootstrap_ci(double_labels[\"A_label\"].values, \n",
    "                                            double_labels[\"B_label\"].values,\n",
    "                                            matthews_corrcoef)[1:])\n",
    "print(bootstrap_ci(double_labels[\"A_label\"].values,\n",
    "                   double_labels[\"B_label\"].values,\n",
    "                   matthews_corrcoef))\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test set (images not used in training CNN)\n",
    "# Remove images that were used in CNN training\n",
    "trg_data = pd.read_csv(\"/home/colin/Downloads/CNN_Full_preds.csv\", \n",
    "                       usecols=[\"patient_id\", \"isDuplicate\"], \n",
    "                       index_col=\"patient_id\")\n",
    "test_data = pred_data[trg_data.loc[:, \"isDuplicate\"] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Sinogram Based Detection Evaluation \"\"\"\n",
    "metrics[\"SBD\"] = get_metrics(pred_data[\"bit_label\"].values, pred_data[\"bhk_pred\"].values)\n",
    "\n",
    "\n",
    "# Compute some stats about this data\n",
    "print(\"Number of Images tested on:\", len(pred_data[\"bhk_pred\"]))\n",
    "print(\"Number of Strong: \", sum(pred_data[\"DA_mag\"] == 2))\n",
    "print(\"Number of Weak: \",   sum(pred_data[\"DA_mag\"] == 1))\n",
    "print(\"Number of no DA: \",  sum(pred_data[\"DA_mag\"] == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Sinogram-Based DA Location Detection and Thresholding DA location detection \"\"\"\n",
    "thr_loc_pred = []   # Contains a location prediction for every DA+ patient\n",
    "sin_loc_pred = []   # COntains a location prediction for every img predictied as DA+ by SBD\n",
    "loc_pids = []\n",
    "loc_labels = []\n",
    "\n",
    "labels = da_labels[\"a_slice\"]\n",
    "\n",
    "thr_test_ids = [k for k in thr_loc_data if int(k) in test_data.index]\n",
    "sin_test_ids = [k for k in sin_loc_data if int(k) in test_data.index]\n",
    "\n",
    "\n",
    "for key in thr_test_ids :\n",
    "    thr_preds = thr_loc_data[key]\n",
    "    sin_preds = sin_loc_data[key]    \n",
    "\n",
    "    label = int(labels.loc[int(key)] )\n",
    "    \n",
    "    # Thresholding location prediction\n",
    "    if len(thr_preds) > 1 :\n",
    "        # Find pred which is closest to label\n",
    "        deltas = np.abs(np.array(thr_preds) - label)\n",
    "        best_pred = thr_preds[np.argmin(deltas)]\n",
    "        thr_loc_pred.append(best_pred)\n",
    "    else :\n",
    "        thr_loc_pred.append(thr_preds[0])\n",
    "        \n",
    "    # Sinogram location prediction\n",
    "    if len(sin_preds) > 1 :\n",
    "        sin_deltas = np.abs(np.array(sin_preds)+20 - label)\n",
    "        sin_best_pred = sin_preds[np.argmin(sin_deltas)]+20\n",
    "        sin_loc_pred.append(best_pred)\n",
    "    elif len(sin_preds) == 1 :\n",
    "        sin_loc_pred.append(sin_preds[0]+20)\n",
    "    else :\n",
    "        sin_loc_pred.append(np.nan)\n",
    "    \n",
    "\n",
    "    loc_pids.append(key)\n",
    "    loc_labels.append(label)\n",
    "\n",
    "    \n",
    "    \n",
    "loc_df = pd.DataFrame({\"patient_id\": loc_pids, \n",
    "                       \"label\": loc_labels, \n",
    "                       \"thr_pred\": thr_loc_pred,\n",
    "                       \"sin_pred\": sin_loc_pred}).set_index(\"patient_id\")\n",
    "\n",
    "loc_df[\"thr_delta\"] = loc_df[\"thr_pred\"] - loc_df[\"label\"]\n",
    "loc_df[\"sin_delta\"] = loc_df[\"sin_pred\"] - loc_df[\"label\"]\n",
    "        \n",
    "print(f\"Thresholding location test images: {len(loc_df['thr_pred'].dropna())}\")\n",
    "print(f\"Sinogram location test images: {len(loc_df['sin_pred'].dropna())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at results\n",
    "n_thr = len(loc_df['thr_delta'].dropna())\n",
    "exact = len(loc_df[loc_df[\"thr_delta\"] == 0])         / n_thr\n",
    "pm_5  = len(loc_df[np.abs(loc_df[\"thr_delta\"]) < 5])  / n_thr\n",
    "pm_10 = len(loc_df[np.abs(loc_df[\"thr_delta\"]) < 10]) / n_thr\n",
    "pm_15 = len(loc_df[np.abs(loc_df[\"thr_delta\"]) < 15]) / n_thr\n",
    "print(\"THRESHOLDING BASED LOCATION\")\n",
    "print(\"Tested on \", len(loc_df[\"thr_pred\"].dropna()), \"images\")\n",
    "print(exact, pm_5, pm_10, pm_15)\n",
    "print(\"Mean:\", loc_df[\"thr_delta\"].mean())\n",
    "print(\"Std:\", loc_df[\"thr_delta\"].std())\n",
    "\n",
    "# Look at sinogram-based results\n",
    "n_sin = len(loc_df[\"sin_delta\"].dropna())\n",
    "exact = len(loc_df[loc_df[\"sin_delta\"] == 0])         / n_sin\n",
    "pm_5  = len(loc_df[np.abs(loc_df[\"sin_delta\"]) < 5])  / n_sin\n",
    "pm_10 = len(loc_df[np.abs(loc_df[\"sin_delta\"]) < 10]) / n_sin\n",
    "pm_15 = len(loc_df[np.abs(loc_df[\"sin_delta\"]) < 15]) / n_sin\n",
    "print(\"\\nSINOGRAM BASED LOCATION\")\n",
    "print(\"Tested on \", len(loc_df[\"sin_pred\"].dropna()), \"images\")\n",
    "print(exact, pm_5, pm_10, pm_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" Make graphs based on above results \"\"\"\n",
    "\n",
    "\n",
    "# # Create list of percentages for each distance\n",
    "# labeller, n_double = [], len(double_labels)\n",
    "# label_diff = double_labels[\"raw_diff\"]\n",
    "# sin_diff, thr_diff = [], []\n",
    "\n",
    "# limit = 40\n",
    "\n",
    "# for i in range(0, limit) :\n",
    "#     if i == 0 :\n",
    "#         labeller.append(len(label_diff[label_diff == i].abs()) / n_double)\n",
    "#         sin_diff.append(len(loc_df[loc_df[\"sin_delta\"].abs() == i]) / n_sin)\n",
    "#         thr_diff.append(len(loc_df[loc_df[\"thr_delta\"].abs() == i]) / n_thr)\n",
    "\n",
    "#     else :\n",
    "#         labeller.append(len(label_diff[label_diff < i].abs()) / n_double)\n",
    "#         sin_diff.append(len(loc_df[loc_df[\"sin_delta\"].abs() < i]) / n_sin)\n",
    "#         thr_diff.append(len(loc_df[loc_df[\"thr_delta\"].abs() < i]) / n_thr)\n",
    "        \n",
    "# # divide all data by number of samples to get percentages\n",
    "# mpl.style.use(\"classic\")\n",
    "# fig = plt.figure()\n",
    "# fig.set_facecolor('white')\n",
    "# plt.plot(np.arange(0, limit), labeller, \"--\", label=\"Labeller Agreement\")\n",
    "# plt.plot(np.arange(0, limit), sin_diff, label=\"Sinogram-based detection\")\n",
    "# plt.plot(np.arange(0, limit), thr_diff, label=\"Threshold-based detection\")\n",
    "# plt.title(\"Comparison of DA slice detection methods with labeller agreement\")\n",
    "# plt.xlabel(\"Distance from prediction to label [slices]\")\n",
    "# plt.ylabel(\"Proportion of Images With Agreeing DA Location\")\n",
    "# plt.legend(loc=\"center right\")\n",
    "# plt.ylim([0, 1])\n",
    "# plt.xlim([0, 25])\n",
    "# plt.xticks(np.arange(0, 25))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Violin plot for location data \"\"\"\n",
    "thr = loc_df[\"thr_delta\"].dropna().values\n",
    "sin = loc_df[\"sin_delta\"].dropna().values\n",
    "agr = double_labels[\"raw_diff\"].values\n",
    "\n",
    "means = [thr.mean(), sin.mean(), agr.mean()]\n",
    "error = [thr.std(), sin.std(), agr.std()]\n",
    "# mpl.style.use(\"classic\")\n",
    "plt.rc('font', family='sans-serif', size=15)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "ax.violinplot([thr, sin, agr], [0,0.5,1], points=300, widths=0.3,\n",
    "                      showmeans=False, showextrema=False, showmedians=True)\n",
    "ax.set_xticks([0,0.5,1])\n",
    "ax.set_xticklabels([\"Threshold-based\", \"Sinogram-based\", \"Annotator agreement\"])\n",
    "ax.set_ylabel(\"Number of slices between prediction and label\")\n",
    "ax.set_title(\"Comparison of DA location detectors\")\n",
    "plt.rc('font', family='serif', size=15)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Seaborn Violin plots \"\"\"\n",
    "# mpl.style.use(\"classic\")\n",
    "# mpl.style.use(\"classic\")\n",
    "plt.rc('font', family='sans-serif', size=15)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=[6, 4])\n",
    "fig.set_facecolor('white')\n",
    "ax = sns.violinplot(data=[thr, sin, agr])\n",
    "ax.set_xticklabels([\"Threshold-based\", \"Sinogram-based\", \"Annotator\\nagreement\"])\n",
    "ax.set_ylabel(\"Slices between prediction and label\")\n",
    "ax.set_xlabel(\"Detection Method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Convolutional Neural Network Detection \"\"\"\n",
    "\n",
    "# # Remove images that were used in CNN training\n",
    "# trg_data = pd.read_csv(\"/home/colin/Downloads/CNN_Full_preds.csv\", \n",
    "#                        usecols=[\"patient_id\", \"isDuplicate\"], \n",
    "#                        index_col=\"patient_id\")\n",
    "# test_data = pred_data[trg_data.loc[:, \"isDuplicate\"] == False]\n",
    "\n",
    "# Print Some stats\n",
    "print(\"Number of CNN test Images Used: \", len(test_data))\n",
    "print(\"Number of Strong: \", sum(test_data[\"DA_mag\"] == 2))\n",
    "print(\"Number of Weak: \", sum(test_data[\"DA_mag\"] == 1))\n",
    "print(\"Number of no DA: \", sum(test_data[\"DA_mag\"] == 0))\n",
    "\n",
    "# Get performance metrics\n",
    "metrics[\"CNN\"] = get_metrics(test_data[\"bit_label\"].values, \n",
    "                             test_data[\"CNN_pred\"].values, \n",
    "                             scores=test_data[\"CNN_prob1\"].values)\n",
    "\n",
    "# mpl.style.use(\"classic\")\n",
    "\n",
    "\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(test_data[\"bit_label\"].values, \n",
    "                                 test_data[\"CNN_prob1\"].values,\n",
    "                                 pos_label=2)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_facecolor('white')\n",
    "plt.rc('font', family='sans-serif', size=15)\n",
    "plt.rc('xtick', labelsize='x-small')\n",
    "plt.rc('ytick', labelsize='x-small')\n",
    "\n",
    "plt.plot(fpr, tpr, label=\"AUC = \"+str(metrics.loc[\"Weighted AUC\", \"CNN\"])[:5])\n",
    "plt.plot([0,0.5, 1], [0, 0.5, 1], label=\"Random\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"Binary CNN ROC Curve\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.show()\n",
    "\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a 3-Class Classifier\n",
    "![alt text](3_class_schematic.jpg \"Schematic for a 3-class DA classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 3-Class DA Classifier Results \"\"\"\n",
    "# Algorithm for 3 class\n",
    "multi_pred = []\n",
    "for pid in test_data.index.values :   # Use CNN test data\n",
    "    sinogram = test_data.loc[pid, \"bhk_pred\"]\n",
    "    cnn = test_data.loc[pid, \"CNN_pred\"]\n",
    "    \n",
    "    # Sinogram first\n",
    "    if sinogram == 2 :\n",
    "        if cnn == 2 : \n",
    "            multi_pred.append(2)\n",
    "        else : # CNN = 0\n",
    "            multi_pred.append(0)\n",
    "    else : # Sinogram = 0\n",
    "        if cnn == 2 :\n",
    "            multi_pred.append(1)\n",
    "        else : # CNN = 0\n",
    "            multi_pred.append(0)\n",
    "            \n",
    "\n",
    "test_data.loc[:, \"multi_pred\"] = multi_pred\n",
    "\n",
    "# Print stats\n",
    "print(\"Number of test images: \", len(test_data))\n",
    "print(\"Number of strong\", sum(test_data[\"DA_mag\"] == 2))\n",
    "print(\"Number of weak\", sum(test_data[\"DA_mag\"] == 1))\n",
    "print(\"Number of none\", sum(test_data[\"DA_mag\"] == 0), \"\\n\")\n",
    "\n",
    "print(\"MCC For 3-Class Classifier\")\n",
    "three_class_mcc, mcc_3c_p = permutation_test(test_data[\"DA_mag\"], \n",
    "                                   test_data[\"multi_pred\"],\n",
    "                                   matthews_corrcoef)\n",
    "mcc_ci = bootstrap_ci(test_data[\"DA_mag\"], test_data[\"multi_pred\"], matthews_corrcoef)[1 : ]\n",
    "print(three_class_mcc, mcc_ci, mcc_3c_p)\n",
    "\n",
    "# Add 3 class MCC to metrics df\n",
    "metrics.loc[\"MCC\", \"3_class\"] = three_class_mcc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.loc[\"MCC CI\", :].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib as mpl\n",
    "# mpl.style.use('fast')\n",
    "# mpl.style.use(\"classic\")\n",
    "\n",
    "\n",
    "plt.close('all')\n",
    "plt.rc('font', family='sans-serif', size=15)\n",
    "plt.rc('xtick', labelsize='x-small')\n",
    "plt.rc('ytick', labelsize='x-small')\n",
    "fig, ax = plt.subplots(figsize=[6, 4], facecolor=\"white\")\n",
    "\n",
    "# metrics.loc[\"MCC\", :].plot.bar(color=[\"#59A14F\", \"#ECC539\", \"#AA6A99\", \"#519DBC\", \"#F08333\"], \n",
    "#                                ax=ax)\n",
    "# plt.errorbar([0, 1, 2, 3, 4], metrics.loc[\"MCC\", :], metrics.loc[\"MCC CI\", :])\n",
    "ax.bar([0, 1, 2, 3, 4], metrics.loc[\"MCC\", :],  metrics.loc[[\"MCC CI_l\",\"MCC CI_u\"], :].values)\n",
    "# metrics.loc[\"MCC\", :].plot.bar()\n",
    "plt.ylabel(\"Matthews Correlation Coeficient (MCC)\", fontsize=15)\n",
    "plt.xlabel(\"Classification Method\", fontsize=15)\n",
    "plt.xticks([0, 1, 2, 3, 4], \n",
    "           [\"SBD\", \"CNN\", \"Hybrid\", \"Annotator\\n(binary)\", \"Annotator\\n(3-class)\"])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create other contingency tables\"\"\"\n",
    "# Labeller agreement\n",
    "print(\"Labeller Agreement\")\n",
    "print(pd.crosstab(double_labels[\"A_label\"], double_labels[\"B_label\"]), \"\\n\")\n",
    "\n",
    "# Sinogram detection\n",
    "print(\"Sinogram binary detection\")\n",
    "print(pd.crosstab(pred_data[\"bit_label\"], pred_data[\"bhk_pred\"]), \"\\n\")\n",
    "\n",
    "# CNN detection\n",
    "print(\"CNN binary detection\")\n",
    "print(pd.crosstab(test_data[\"bit_label\"], test_data[\"CNN_pred\"] ), \"\\n\")\n",
    "\n",
    "# Hybrid 3-class detection\n",
    "print(\"Hybrid 3-class detection\")\n",
    "print(pd.crosstab(test_data[\"DA_mag\"], test_data[\"multi_pred\"]) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radcure",
   "language": "python",
   "name": "radcure"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
